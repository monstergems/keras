# -*- coding: utf-8 -*-
"""Assignment1-Task2.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/173H-1rbJS8QNL8wXQcsDhjBAYOtzKf5G
"""

pip install tensorflow

import numpy as np
import tensorflow as tf
from tensorflow.keras import layers, models
from sklearn.model_selection import train_test_split
from tensorflow import keras
from tensorflow.keras.layers import BatchNormalization
from tensorflow.keras.optimizers import Adam
from keras.regularizers import l2,l1
from sklearn.metrics import accuracy_score
import matplotlib.pyplot as plt
import keras
from keras.models import Sequential, Model
from keras.layers import Dense, Dropout, Flatten, Input
from keras.layers import Conv2D, MaxPooling2D

train_loss_history = []
val_loss_history = []
train_accuracy_history = []
val_accuracy_history = []

"""#Classification

Importing the data
"""

clock_images = np.load("images.npy")
clock_labels = np.load("labels.npy")

"""Checking the range of the labels"""

min_hour = np.min(clock_labels[:, 0])
max_hour = np.max(clock_labels[:, 0])
min_minute = np.min(clock_labels[:, 1])
max_minute = np.max(clock_labels[:, 1])

print("Minimum Hour:", min_hour, "Maximum Hour:", max_hour)
print("Minimum Minute:", min_minute, "Maximum Minute:", max_minute)

"""Normalizing the data"""

normalized_clock_images = clock_images /255.0

"""Splitting and shuffling"""

x_train, x_test, y_train, y_test = train_test_split(normalized_clock_images, clock_labels, test_size = 0.2, random_state = 42, shuffle = True, stratify = clock_labels)
x_train, x_val, y_train, y_val = train_test_split(x_train, y_train, test_size = 0.1, random_state = 42, shuffle = True, stratify = y_train)

"""Grouping the labels"""

def group_clock_labels(data, interval):
  grouped_clock_labels = []
  for label in data:
    hours, minutes = label
    label_group = (hours * 60 + minutes) // interval
    grouped_clock_labels.append(label_group)
  grouped_clock_labels = np.array(grouped_clock_labels)
  return grouped_clock_labels

#Setting the interval [30, 15, 10, 1]
grouped_clock_labels_train = group_clock_labels(y_train, 1)
grouped_clock_labels_val = group_clock_labels(y_val, 1)
grouped_clock_labels_test = group_clock_labels(y_test, 1)

"""Visualizing images and their labels"""

import matplotlib.pyplot as plt
for i in range(5):
    plt.subplot(2, 5, i + 1)
    plt.imshow(x_val[i], cmap='gray')
    plt.title(grouped_clock_labels_val[i])
    plt.axis('off')

plt.tight_layout()
plt.show()

"""Defining the Classification Model"""

def classification_model(num_of_classes):

    images_shape = (75, 75, 1)
    model = keras.Sequential(
        [
            keras.Input(shape = images_shape),
            layers.BatchNormalization(),
            layers.Conv2D(16, kernel_size = (3, 3), activation = "relu"),
            layers.MaxPooling2D(pool_size = (2, 2)),

            layers.BatchNormalization(),
            layers.Conv2D(32, kernel_size = (3, 3), activation = "relu"),
            layers.MaxPooling2D(pool_size = (2, 2)),

            layers.BatchNormalization(),
            layers.Conv2D(64, kernel_size = (3, 3), activation = "relu"),
            layers.MaxPooling2D(pool_size = (2, 2)),
            layers.Dropout(0.2),

            layers.Flatten(),

            layers.BatchNormalization(),
            layers.Dense(512, activation = "relu"),
            layers.BatchNormalization(),
            layers.Dense(256, activation = "relu"),
            layers.Dense(num_of_classes, activation = "softmax"),
        ]
    )
    model.summary()

    return model

"""24 classes - interval is set to 30 in the group_clock_labels function

"""

classification_24 = classification_model(num_of_classes = 24)
classification_24.compile(optimizer = 'adam',  loss = ['sparse_categorical_crossentropy'],  metrics = ['accuracy'])
history_classification_24 = classification_24.fit(x_train, grouped_clock_labels_train, validation_data = (x_val, grouped_clock_labels_val), batch_size = 32,  epochs = 10)
test_loss_classification_24, test_accuracy_classification_24 = classification_24.evaluate(x_test, grouped_clock_labels_test, verbose = 1)
print("Accuracy of the Classification Model with 24 classes for the test set is:", test_accuracy_classification_24)

train_loss_classification_24 = history_classification_24.history['loss']
val_loss_classification_24 = history_classification_24.history['val_loss']
train_accuracy_classification_24 = history_classification_24.history['accuracy']
val_accuracy_classification_24 = history_classification_24.history['val_accuracy']

"""48 classes - interval is set to 15 in the group_clock_labels function"""

classification_48 = classification_model(num_of_classes = 48)
classification_48.compile(optimizer = 'adam',  loss = ['sparse_categorical_crossentropy'],  metrics = ['accuracy'])
history_classification_48 = classification_48.fit(x_train, grouped_clock_labels_train, validation_data = (x_val, grouped_clock_labels_val), batch_size = 32,  epochs = 10)
test_loss_classification_48, test_accuracy_classification_48 = classification_48.evaluate(x_test, grouped_clock_labels_test, verbose = 1)
print("Accuracy of the Classification Model with 48 classes for the test set is:", test_accuracy_classification_48)

train_loss_classification_48 = history_classification_48.history['loss']
val_loss_classification_48 = history_classification_48.history['val_loss']
train_accuracy_classification_48 = history_classification_48.history['accuracy']
val_accuracy_classification_48 = history_classification_48.history['val_accuracy']

"""72 classes - interval is set to 10 in the group_clock_labels function

"""

classification_72 = classification_model(num_of_classes = 72)
classification_72.compile(optimizer = 'adam',  loss = ['sparse_categorical_crossentropy'],  metrics = ['accuracy'])
history_classification_72 = classification_72.fit(x_train, grouped_clock_labels_train, validation_data = (x_val, grouped_clock_labels_val), batch_size = 32,  epochs = 10)
test_loss_classification_72, test_accuracy_classification_72 = classification_72.evaluate(x_test, grouped_clock_labels_test, verbose = 1)
print("Accuracy of the Classification Model with 72 classes for the test set is:", test_accuracy_classification_72)

train_loss_classification_72 = history_classification_72.history['loss']
val_loss_classification_72 = history_classification_72.history['val_loss']
train_accuracy_classification_72 = history_classification_72.history['accuracy']
val_accuracy_classification_72 = history_classification_72.history['val_accuracy']

"""720 classes - interval is set to 1 in the group_clock_labels function"""

classification_720 = classification_model(num_of_classes = 720)
classification_720.compile(optimizer = 'adam',  loss = ['sparse_categorical_crossentropy'],  metrics = ['accuracy'])
history_classification_720 = classification_720.fit(x_train, grouped_clock_labels_train, validation_data = (x_val, grouped_clock_labels_val), batch_size = 32,  epochs = 10)
test_loss_classification_720, test_accuracy_classification_720 = classification_720.evaluate(x_test, grouped_clock_labels_test, verbose = 1)
print("Accuracy of the Classification Model with 720 classes for the test set is:", test_accuracy_classification_720)

train_loss_classification_720 = history_classification_720.history['loss']
val_loss_classification_720 = history_classification_720.history['val_loss']
train_accuracy_classification_720 = history_classification_720.history['accuracy']
val_accuracy_classification_720 = history_classification_720.history['val_accuracy']

train_loss_history.append(train_loss_classification_24)
train_loss_history.append(train_loss_classification_48)
train_loss_history.append(train_loss_classification_72)
train_loss_history.append(train_loss_classification_720)
val_loss_history.append(val_loss_classification_24)
val_loss_history.append(val_loss_classification_48)
val_loss_history.append(val_loss_classification_72)
val_loss_history.append(val_loss_classification_720)
train_accuracy_history.append(train_accuracy_classification_24)
train_accuracy_history.append(train_accuracy_classification_48)
train_accuracy_history.append(train_accuracy_classification_72)
train_accuracy_history.append(train_accuracy_classification_720)
val_accuracy_history.append(val_accuracy_classification_24)
val_accuracy_history.append(val_accuracy_classification_48)
val_accuracy_history.append(val_accuracy_classification_72)
val_accuracy_history.append(val_accuracy_classification_720)

alpha = 0.7
smoothed_train_loss = []
for loss in train_loss_history:
    smoothed_train = [loss[0]]
    for i in range(1, len(loss)):
        smoothed_train.append(alpha * loss[i] + (1 - alpha) * smoothed_train[i - 1])
    smoothed_train_loss.append(smoothed_train)


num_of_classes_list = [24, 48, 72, 720]
for i, num_class in enumerate(num_of_classes_list):
    plt.plot(smoothed_train_loss[i], label=f'Number of Classes: {num_class}')

plt.xlabel('Epochs')
plt.ylabel('Training Loss')
plt.legend()
plt.show()

alpha = 0.2
smoothed_val_loss = []
for loss in val_loss_history:
    smoothed_val = [loss[0]]
    for i in range(1, len(loss)):
        smoothed_val.append(alpha * loss[i] + (1 - alpha) * smoothed_val[i - 1])
    smoothed_val_loss.append(smoothed_val)


num_of_classes_list = [24, 48, 72, 720]
for i, num_class in enumerate(num_of_classes_list):
    plt.plot(smoothed_val_loss[i], label=f'Number of Classes: {num_class}')

plt.xlabel('Epochs')
plt.ylabel('Validation Loss')
plt.legend()
plt.show()

alpha = 0.6
smoothed_train_acc = []
for accuracy in train_accuracy_history:
    smoothed_train = [accuracy[0]]
    for i in range(1, len(accuracy)):
        smoothed_train.append(alpha * accuracy[i] + (1 - alpha) * smoothed_train[i - 1])
    smoothed_train_acc.append(smoothed_train)


num_of_classes_list = [24, 48, 72, 720]
for i, num_class in enumerate(num_of_classes_list):
    plt.plot(smoothed_train_acc[i], label=f'Number of Classes: {num_class}')

plt.xlabel('Epochs')
plt.ylabel('Training Accuracy')
plt.legend()
plt.show()

alpha = 0.3
smoothed_val_acc = []
for accuracy in val_accuracy_history:
    smoothed_val = [accuracy[0]]
    for i in range(1, len(accuracy)):
        smoothed_val.append(alpha * accuracy[i] + (1 - alpha) * smoothed_val[i - 1])
    smoothed_val_acc.append(smoothed_val)


num_of_classes_list = [24, 48, 72, 720]
for i, num_class in enumerate(num_of_classes_list):
    plt.plot(smoothed_val_acc[i], label=f'Number of Classes: {num_class}')

plt.xlabel('Epochs')
plt.ylabel('Validation Accuracy')
plt.legend()
plt.show()

"""#######################################################################################################

#Regression
"""

clock_images = np.load("images.npy")
clock_labels = np.load("labels.npy")

"""Normalizing the data"""

normalized_clock_images = clock_images /255.0

"""Splitting and shuffling the data"""

x_train, x_test, y_train, y_test = train_test_split(normalized_clock_images, clock_labels, test_size = 0.2, random_state = 42, shuffle = True, stratify = clock_labels)
x_train, x_val, y_train, y_val = train_test_split(x_train, y_train, test_size = 0.1, random_state = 42, shuffle = True, stratify = y_train)

"""Create regression labels (categorical labels of hours and minutes get transformed to a single continuous value)"""

def regression_labels(data):
  regression_clock_labels = []
  for label in data:
    hours, minutes = label
    regression_label = round(hours + minutes / 60, 2)
    regression_clock_labels.append(regression_label)

  regression_clock_labels = np.array(regression_clock_labels,dtype="float32")
  return regression_clock_labels

grouped_regression_train = regression_labels(y_train)
grouped_regression_test = regression_labels(y_test)
grouped_regression_val = regression_labels(y_val)

"""Visualizing images and their labels"""

import matplotlib.pyplot as plt
for i in range(5):
    plt.subplot(2, 5, i + 1)
    plt.imshow(x_test[i], cmap='gray')
    plt.title(grouped_regression_test[i])
    plt.axis('off')

plt.tight_layout()
plt.show()

def regression_model():

    images_shape = (75, 75, 1)
    model = keras.Sequential(
        [
            keras.Input(shape = images_shape),
            layers.BatchNormalization(),
            layers.Conv2D(16, kernel_size = (3, 3), activation = "relu"),
            layers.MaxPooling2D(pool_size = (2, 2)),


            layers.BatchNormalization(),
            layers.Conv2D(32, kernel_size = (3, 3), activation = "relu"),
            layers.MaxPooling2D(pool_size = (2, 2)),


            layers.BatchNormalization(),
            layers.Conv2D(64, kernel_size = (3, 3), activation = "relu"),
            layers.MaxPooling2D(pool_size = (2, 2)),
            layers.Dropout(0.2),

            layers.Flatten(),

            layers.Dense(512, activation = "relu"),
            layers.BatchNormalization(),
            layers.Dense(256, activation = "relu"),
            layers.BatchNormalization(),
            layers.Dense(1, activation = "relu"),
        ]
    )
    model.summary()

    return model

def common_sense_error(y_true, y_pred):
    different_value = tf.math.abs(y_true - y_pred)
    minimum_different = tf.minimum(different_value, 12 - different_value)
    return tf.reduce_mean(minimum_different)

regression_model = regression_model()
regression_model.compile(loss= ["mean_squared_error",common_sense_error] , optimizer="adam", metrics=["mean_squared_error",common_sense_error])
regression_history = regression_model.fit(x_train, grouped_regression_train, validation_data = (x_val, grouped_regression_val), batch_size = 32,  epochs = 10)

regression_result = regression_model.evaluate(x_test, grouped_regression_test, verbose = 1)
print("The value of Mean Squared Error for the test set is: {:.2f}, And the value of Common Sense Error is: {:.2f}".format(regression_result[1], regression_result[2]))

train_mse = regression_history.history['mean_squared_error']
train_cse = regression_history.history['common_sense_error']
val_mse = regression_history.history['val_mean_squared_error']
val_cse = regression_history.history['val_common_sense_error']

alpha = 0.5
smoothed_val_cse = [val_cse[0]]
smoothed_val_mse = [val_mse[0]]


for i in range(1, len(val_cse)):
    smoothed_val_cse.append(alpha * val_cse[i] + (1 - alpha) * smoothed_val_cse[i - 1])
    smoothed_val_mse.append(alpha * val_mse[i] + (1 - alpha) * smoothed_val_mse[i - 1])

alpha2 = 0.8
smoothed_train_cse = [train_cse[0]]
smoothed_train_mse = [train_mse[0]]


for i in range(1, len(train_cse)):
    smoothed_train_cse.append(alpha2 * train_cse[i] + (1 - alpha2) * smoothed_train_cse[i - 1])
    smoothed_train_mse.append(alpha2 * train_mse[i] + (1 - alpha2) * smoothed_train_mse[i - 1])

plt.figure(figsize=(10, 6))

plt.subplot(2, 2, 1)
plt.plot(smoothed_train_mse, label='Mean Squared Error for train set')
plt.xlabel('Epochs')
plt.ylabel('Mean Squared Error')
plt.title('Mean Squared Error for train set')



plt.subplot(2, 2, 2)
plt.plot(smoothed_train_cse, label='Common Sense Error for train set')
plt.xlabel('Epochs')
plt.ylabel('Common Sense Error')
plt.title('Common Sense Error for train set')


plt.subplot(2, 2, 3)
plt.plot(smoothed_val_mse, label='Mean Squared Error for validation set')
plt.xlabel('Epochs')
plt.ylabel('Mean Squared Error')
plt.title('Mean Squared Error for validation set')



plt.subplot(2, 2, 4)
plt.plot(smoothed_val_cse, label='Common Sense Error for validation set')
plt.xlabel('Epochs')
plt.ylabel('Common Sense Error')
plt.title('Common Sense Error for validation set')


plt.tight_layout()
plt.show()

"""#Multihead Model"""

clock_images = np.load("images.npy")
clock_labels = np.load("labels.npy")

"""Normalizing the data"""

clock_images = clock_images / 255.0

x_train, x_test, y_train, y_test = train_test_split(clock_images,clock_labels, test_size = 0.2, random_state = 42, shuffle = True, stratify = clock_labels)
x_train, x_val, y_train, y_val = train_test_split(x_train, y_train, test_size = 0.1, random_state = 42, shuffle = True, stratify = y_train)

def multi_head_model_labels(data):
  multi_head_label_hour = []
  multi_head_label_minute = []
  for label in data:
    hours, minutes = label
    multi_head_hour = hours
    multi_head_minute = round((minutes / 60),2)
    multi_head_label_hour.append(multi_head_hour)
    multi_head_label_minute.append(multi_head_minute)

  multi_head_label_hour = np.array(multi_head_label_hour)
  multi_head_label_minute = np.array(multi_head_label_minute)

  return multi_head_label_hour, multi_head_label_minute

multi_head_label_hour_train, multi_head_label_minute_train = multi_head_model_labels(y_train)
multi_head_label_hour_test, multi_head_label_minute_test = multi_head_model_labels(y_test)
multi_head_label_hour_val, multi_head_label_minute_val= multi_head_model_labels(y_val)

"""Visualizing the labels"""

for i in range(5):
    plt.subplot(2, 5, i + 1)
    plt.imshow(x_val[i], cmap='gray')
    plt.title(f"Hour: {multi_head_label_hour_val[i]}\nMinute: {multi_head_label_minute_val[i]}")
    plt.axis('off')

plt.tight_layout()
plt.show()

"""Creating the multi head model"""

def multihead_model():

  input_shape = [75, 75, 1]

  input = keras.Input(input_shape)
  conv1 = layers.Conv2D(64, kernel_size = (3, 3), activation = "relu")(input)
  maxpool1 = layers.MaxPooling2D()(conv1)


  conv2 = layers.Conv2D(64, kernel_size= (3,3), activation='relu')(maxpool1)
  maxpool2 = layers.MaxPooling2D(pool_size=(2, 2))(conv2)


  conv3 = layers.Conv2D(64, kernel_size = (3,3), activation = 'relu')(maxpool2)
  maxpool3 = layers.MaxPooling2D(pool_size = (2, 2))(conv3)

  flat = layers.Flatten()(maxpool3)


  hours = layers.Dense(128, activation = 'relu')(flat)
  hours = layers.Dense(12, activation = 'softmax', name = 'hours')(hours)


  minutes = layers.Dense(128, activation = 'relu')(flat)
  minutes = layers.Dense(1, activation = 'relu', name = 'minutes')(minutes)


  multi_head_model = keras.Model(inputs = input, outputs = [hours, minutes])
  multi_head_model.summary()
  return multi_head_model

multi_head_model = multihead_model()
multi_head_model.compile(optimizer = 'adam',  loss = ['sparse_categorical_crossentropy','mean_squared_error'],  metrics = ['accuracy','mean_squared_error'])

multi_head_model_history = multi_head_model.fit(x_train, [multi_head_label_hour_train, multi_head_label_minute_train], validation_data = (x_val, [multi_head_label_hour_val, multi_head_label_minute_val]), batch_size = 32,  epochs = 10)

multi_head_model_test = multi_head_model.evaluate(x_test, [multi_head_label_hour_test, multi_head_label_minute_test], verbose = 1)
print("Accuracy of the hour accuracy for the test set is:",multi_head_model_test [3])
print("Minutes Mean Squared Error for the test set is:",multi_head_model_test [6])

hours_accuracy_train_multi_head_model = multi_head_model_history.history['hours_accuracy']
hours_accuracy_val_multi_head_model = multi_head_model_history.history['val_hours_accuracy']
minutes_mse_train_multi_head_model = multi_head_model_history.history['minutes_mean_squared_error']
minutes_mse_val_multi_head_model = multi_head_model_history.history['val_minutes_mean_squared_error']

alpha = 0.3
smoothed_train_mse_multihead = [minutes_mse_train_multi_head_model[0]]
smoothed_val_mse_multihead = [minutes_mse_val_multi_head_model[0]]

for i in range(1, len(minutes_mse_train_multi_head_model)):
    smoothed_train_mse_multihead.append(alpha * minutes_mse_train_multi_head_model[i] + (1 - alpha) * smoothed_train_mse_multihead[i - 1])
    smoothed_val_mse_multihead.append(alpha * minutes_mse_val_multi_head_model[i] + (1 - alpha) * smoothed_val_mse_multihead[i - 1])

alpha2 = 0.6
smoothed_train_accuracy_multihead = [hours_accuracy_train_multi_head_model[0]]
smoothed_val_train_multihead = [hours_accuracy_val_multi_head_model[0]]

for i in range(1, len(hours_accuracy_train_multi_head_model)):
    smoothed_train_accuracy_multihead.append(alpha2 * hours_accuracy_train_multi_head_model[i] + (1 - alpha2) * smoothed_train_accuracy_multihead[i - 1])
    smoothed_val_train_multihead.append(alpha2 * hours_accuracy_val_multi_head_model[i] + (1 - alpha2) * smoothed_val_train_multihead[i - 1])

plt.figure(figsize = (10,6))
plt.subplot(2, 2, 1)
plt.plot(smoothed_train_accuracy_multihead, label='Hours Accuracy for train set')
plt.xlabel('Epochs')
plt.ylabel('Accuracy')
plt.title('Hours Accuracy for train set')

plt.subplot(2, 2, 2)
plt.plot(smoothed_train_mse_multihead, label='Minutes Mean Squared Error for train set')
plt.xlabel('Epochs')
plt.ylabel('Mean Squared Error')
plt.title('Mean Squared Error for train set')


plt.subplot(2, 2, 3)
plt.plot(smoothed_val_train_multihead, label='Hours Accuracy for validation set')
plt.xlabel('Epochs')
plt.ylabel('Accuracy')
plt.title('Hours Accuracy for validation set')


plt.subplot(2, 2, 4)
plt.plot(smoothed_val_mse_multihead, label='Minutes Mean Squared Error for validation set')
plt.xlabel('Epochs')
plt.ylabel('Mean Squared Error')
plt.title('Mean Squared Error for validation set')


plt.tight_layout()
plt.show()

"""Calculating the Common Sense Error"""

def calculate_common_sense_error(data, multi_head_labels_hours, multi_head_labels_minutes):
  predicted_sum = []
  true_sum = []
  prediction_hours, prediction_minutes = multi_head_model.predict(data)

  for i in range(len(data)):
    predicted_hours = np.argmax(prediction_hours[i]) * 60
    predicted_minutes = (prediction_minutes[i][0]) * 60

    sum_predicted = predicted_hours + predicted_minutes
    predicted_sum.append(sum_predicted)

    true_hours = multi_head_labels_hours[i] * 60
    true_minutes = multi_head_labels_minutes[i] * 60

    sum_true = true_hours + true_minutes
    true_sum.append(sum_true)


  different_val = np.absolute(np.subtract(true_sum, predicted_sum))
  results = np.minimum(np.subtract(720, different_val), different_val)
  return(np.mean(results))

print("The Common Sense error for the Train set is:", round(calculate_common_sense_error(x_train, multi_head_label_hour_train, multi_head_label_minute_train),2))
print("The Common Sense error for the Test set is:", round(calculate_common_sense_error(x_test, multi_head_label_hour_test, multi_head_label_minute_test),2))
print("The Common Sense error for the Validation set is:", round(calculate_common_sense_error(x_val, multi_head_label_hour_val, multi_head_label_minute_val),2))